# Embodied RL Experiments

ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒç¦»æ•£å’Œè¿ç»­æ§åˆ¶çš„å¤šç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

## ç‰¹æ€§

- ğŸ¯ **æ¨¡å—åŒ–æ¶æ„**: è½»æ¾åˆ‡æ¢ç®—æ³•ã€ç¼“å†²åŒºå’Œæ¢ç´¢ç­–ç•¥
- ğŸ”Œ **å¯æ’æ‹”ç»„ä»¶**: æ··åˆæ­é…è®­ç»ƒå™¨ã€å›æ”¾ç¼“å†²åŒºå’Œæ¢ç´¢æ–¹æ³•
- ğŸš€ **å¤šç§ç®—æ³•**: DQNã€Double DQNã€DDPGã€TD3ï¼Œæ˜“äºæ‰©å±•åˆ° Dueling DQNã€Rainbowã€SACã€PPO ç­‰
- ğŸ“Š **å†…ç½®å¯è§†åŒ–**: è®­ç»ƒè¿›åº¦å›¾è¡¨
- âš™ï¸ **é…ç½®ç®¡ç†**: ä½¿ç”¨ draccus è¿›è¡Œé…ç½®ç®¡ç†
- âœ… **ç±»å‹å®‰å…¨**: å…¨é¢çš„ç±»å‹æç¤º
- ğŸ§ª **å¯æµ‹è¯•**: ä¾èµ–æ³¨å…¥ä½¿å•å…ƒæµ‹è¯•å˜å¾—ç®€å•

## å®‰è£…

æœ¬é¡¹ç›®ä½¿ç”¨ `uv` è¿›è¡Œä¾èµ–ç®¡ç†ï¼š

```bash
# å¦‚æœå°šæœªå®‰è£… uvï¼Œå…ˆå®‰è£…
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£…ä¾èµ–
uv sync

# å®‰è£…å¼€å‘ä¾èµ–ï¼ˆåŒ…æ‹¬ ruffï¼‰
uv sync --dev
```

ä¹Ÿæ”¯æŒä½¿ç”¨ `conda` ç®¡ç†ä¾èµ–ï¼š

```bash
conda env create -f environment.yml
conda activate embodied-exps
```

æˆ–è€…ç›´æ¥ä½¿ç”¨ `pip`ï¼š

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

> æ³¨æ„ï¼šå¦‚æœä¸æ˜¯ä½¿ç”¨ `uv` ç®¡ç†é¡¹ç›®çš„è¯ï¼Œå°†ä¸‹è¿°å‘½ä»¤ä¸­çš„ `uv run` æ›¿æ¢æˆ `python` å³å¯ã€‚

### è®­ç»ƒç¦»æ•£æ§åˆ¶ç®—æ³•ï¼ˆDQN ç³»åˆ—ï¼‰

```bash
# è®­ç»ƒ DQNï¼ˆCartPole ç¯å¢ƒï¼‰
uv run scripts/train_dqn.py

# ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°
uv run scripts/train_dqn.py --n_episodes 2000 --gamma 0.99

# è®­ç»ƒ Double DQN
uv run scripts/train_ddqn.py

# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
uv run scripts/test_dqn.py --config_path checkpoints/dqn/<timestamp>/config.json --ckpt_path model_ep800.pth
```

### è®­ç»ƒè¿ç»­æ§åˆ¶ç®—æ³•ï¼ˆDDPG/TD3ï¼‰

```bash
# è®­ç»ƒ DDPGï¼ˆAntBulletEnv ç¯å¢ƒï¼‰
uv run scripts/train_ddpg.py

# è®­ç»ƒ TD3ï¼ˆPendulum ç¯å¢ƒï¼‰
uv run scripts/train_td3.py --env_name Pendulum-v1 --n_episodes 100

# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
uv run scripts/test_td3.py --config_path checkpoints/td3/<timestamp>/config.json
```

## æ¶æ„

æ¡†æ¶å»ºç«‹åœ¨ä»¥ä¸‹æ ¸å¿ƒæŠ½è±¡ä¹‹ä¸Šï¼š

1. **æ™ºèƒ½ä½“** (`BaseAgent`): å®ç°å­¦ä¹ ç®—æ³•ï¼ˆDQNã€DDQNã€DDPGã€TD3 ç­‰ï¼‰
2. **å›æ”¾ç¼“å†²åŒº** (`BaseBuffer`): ç®¡ç†ç»éªŒå­˜å‚¨å’Œé‡‡æ ·
3. **æ¢ç´¢ç­–ç•¥** (`BaseExplorationStrategy`): æ§åˆ¶åŠ¨ä½œé€‰æ‹©
4. **è®­ç»ƒå™¨** (`OffPolicyTrainer`): é€šç”¨çš„ç¦»ç­–ç•¥è®­ç»ƒå¾ªç¯
5. **è¯„ä¼°å™¨** (`OffPolicyEvaluator`): æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

## é¡¹ç›®ç»“æ„

```
rl_models/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ base.py                   # æŠ½è±¡åŸºç±»ï¼ˆBaseAgent, BaseBuffer, BaseExplorationStrategyï¼‰
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ dqn.py                    # DQN å®ç°
â”‚   â”œâ”€â”€ ddqn.py                   # Double DQN å®ç°
â”‚   â”œâ”€â”€ ddpg.py                   # DDPG å®ç°
â”‚   â””â”€â”€ td3.py                    # TD3 å®ç°
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ replay_buffer.py          # å›æ”¾ç¼“å†²åŒºå®ç°ï¼ˆæ™®é€š/ä¼˜å…ˆçº§ï¼‰
â”‚   â”œâ”€â”€ sum_tree.py               # ä¼˜å…ˆçº§å›æ”¾ä½¿ç”¨çš„ SumTree
â”‚   â”œâ”€â”€ logger.py                 # æ—¥å¿—å·¥å…·
â”‚   â””â”€â”€ utils.py                  # å·¥å…·å‡½æ•°
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ common_config.py          # é€šç”¨é…ç½®
â”‚   â”œâ”€â”€ dqn_config.py             # DQN é…ç½®
â”‚   â”œâ”€â”€ ddqn_config.py            # DDQN é…ç½®
â”‚   â”œâ”€â”€ ddpg_config.py            # DDPG é…ç½®
â”‚   â””â”€â”€ td3_config.py             # TD3 é…ç½®
â”œâ”€â”€ nets/
â”‚   â”œâ”€â”€ dqn_models.py             # DQN ç½‘ç»œæ¶æ„
â”‚   â”œâ”€â”€ dqpg_models.py            # DDPG/TD3 ç½‘ç»œæ¶æ„
â”‚   â””â”€â”€ mlp.py                    # é€šç”¨ MLP æ„å»ºå™¨
â”œâ”€â”€ runner/
â”‚   â”œâ”€â”€ trainer.py                # é€šç”¨ç¦»ç­–ç•¥è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ evaluator.py              # æ¨¡å‹è¯„ä¼°å™¨
â”‚   â””â”€â”€ recorder.py               # æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜
â””â”€â”€ exploration.py                # æ¢ç´¢ç­–ç•¥ï¼ˆEpsilon-Greedy, Gaussian Noise ç­‰ï¼‰

scripts/
â”œâ”€â”€ train_dqn.py                  # DQN è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_ddqn.py                 # Double DQN è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_ddpg.py                 # DDPG è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_td3.py                  # TD3 è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_dqn.py                   # DQN æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_ddqn.py                  # Double DQN æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_ddpg.py                  # DDPG æµ‹è¯•è„šæœ¬
â””â”€â”€ test_td3.py                   # TD3 æµ‹è¯•è„šæœ¬

checkpoints/                      # æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæ—¥å¿—
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬è®­ç»ƒæµç¨‹

```python
import draccus
from rl_models.algorithms import DQN
from rl_models.common.replay_buffer import ReplayBuffer
from rl_models.configs import DQNConfig
from rl_models.envs import make_env
from rl_models.exploration import EpsilonGreedyStrategy
from rl_models.runner.trainer import OffPolicyTrainer

# è§£æé…ç½®
config = draccus.parse(DQNConfig)

# åˆ›å»ºç¯å¢ƒ
env = make_env(config.env_name)

# åˆ›å»ºæ™ºèƒ½ä½“
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
agent = DQN(state_dim, action_dim, config)

# åˆ›å»ºå›æ”¾ç¼“å†²åŒº
buffer = ReplayBuffer(max_size=config.buffer_size)

# åˆ›å»ºæ¢ç´¢ç­–ç•¥
exploration_strategy = EpsilonGreedyStrategy(
    epsilon_start=config.epsilon_start,
    epsilon_end=config.epsilon_end,
    epsilon_decay=config.epsilon_decay,
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = OffPolicyTrainer(
    agent=agent,
    env=env,
    buffer=buffer,
    exploration_strategy=exploration_strategy,
    config=config,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### ä½¿ç”¨ä¸åŒçš„ç»„ä»¶

```python
# ä½¿ç”¨ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒº
from rl_models.common.replay_buffer import PrioritizedReplayBuffer
buffer = PrioritizedReplayBuffer(max_size=config.buffer_size)

# ä½¿ç”¨ TD3 ç®—æ³•ï¼ˆè¿ç»­æ§åˆ¶ï¼‰
from rl_models.algorithms import TD3
from rl_models.exploration import GaussianNoiseStrategy

agent = TD3(state_dim, action_dim, max_action, config)
exploration_strategy = GaussianNoiseStrategy(
    action_dim=action_dim,
    max_action=max_action,
    sigma=0.1,
)

# ä½¿ç”¨è´ªå©ªç­–ç•¥ï¼ˆç”¨äºè¯„ä¼°ï¼‰
from rl_models.exploration import GreedyStrategy
exploration = GreedyStrategy()
```

## æ‰©å±•æ¡†æ¶

### æ·»åŠ æ–°ç®—æ³•

```python
from rl_models.core.base import BaseAgent

class MyNewAlgorithm(BaseAgent):
    def __init__(self, state_dim: int, action_dim: int, config):
        super().__init__(config)
        # åˆå§‹åŒ–ç½‘ç»œã€ä¼˜åŒ–å™¨ç­‰
    
    def act(self, state, deterministic=False):
        # åŠ¨ä½œé€‰æ‹©é€»è¾‘
        pass
    
    def update(self, batch):
        # å­¦ä¹ ç®—æ³•
        return {"loss": loss_value}
    
    def state_dict(self):
        # è¿”å›éœ€è¦ä¿å­˜çš„å‚æ•°
        pass
    
    def load_state_dict(self, state_dict):
        # åŠ è½½å‚æ•°
        pass
```

### æ·»åŠ æ–°æ¢ç´¢ç­–ç•¥

```python
from rl_models.core.base import BaseExplorationStrategy

class BoltzmannExploration(BaseExplorationStrategy):
    def __init__(self, temperature=1.0):
        self.temperature = temperature
    
    def select_action(self, state, action_selector, env_action_space):
        # åŸºäº Softmax çš„åŠ¨ä½œé€‰æ‹©
        pass
    
    def update(self):
        # æ›´æ–°æ¸©åº¦å‚æ•°
        pass
```

## å¼€å‘

### ä»£ç è´¨é‡

```bash
# æ ¼å¼åŒ–ä»£ç 
uv run ruff format .

# ä»£ç æ£€æŸ¥
uv run ruff check .

# è‡ªåŠ¨ä¿®å¤å¯ä¿®å¤çš„é—®é¢˜
uv run ruff check . --fix
```

## é…ç½®

æ‰€æœ‰è®­ç»ƒå‚æ•°å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é…ç½®ï¼š

1. **å‘½ä»¤è¡Œå‚æ•°**: `uv run script.py --n_episodes 2000 --gamma 0.99`
3. **é…ç½®æ–‡ä»¶**ï¼š `uv run script.py --config_path your_cfg.json` ä¹Ÿå¯ä»¥æ˜¯ `.yaml` æ–‡ä»¶  
2. **Python dataclass**: åœ¨ `rl_models/configs/` ä¸­ä¿®æ”¹å¯¹åº”çš„é…ç½®ç±»

é…ç½®ç¤ºä¾‹ï¼ˆDQNï¼‰ï¼š

```python
@dataclass
class DQNConfig(CommonConfig):
    exp_name: str = "DQN_CartPole"
    env_name: str = "CartPole-v1"
    batch_size: int = 64
    n_episodes: int = 1000
    gamma: float = 0.95
    learning_rate: float = 0.001
    epsilon_start: float = 1.0
    epsilon_decay: float = 0.995
    epsilon_end: float = 0.01
    buffer_size: int = 2000
    use_prioritized_replay: bool = True
```

## æ£€æŸ¥ç‚¹

è®­ç»ƒåæ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `checkpoints/` ç›®å½•ï¼š

```bash
checkpoints/
â”œâ”€â”€ dqn/
â”‚   â””â”€â”€ 20251121-1642/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model_ep800.pth
â”‚       â””â”€â”€ model_last.pth
â”œâ”€â”€ ddqn/
â”œâ”€â”€ ddpg/
â””â”€â”€ td3/
    â””â”€â”€ 20251121-1929/
        â”œâ”€â”€ config.json
        â””â”€â”€ model_last.pth
```

æ£€éªŒæ¨¡å‹ï¼š

```bash
# if --ckpt_path doesn't specify, it will choose model_last.pth in the config_path directory
uv run scripts/test_xxx.py --config_path your_json_yaml_path --ckpt_path your_ckpt_path
```

## æ”¯æŒçš„ç®—æ³•

### ç¦»æ•£æ§åˆ¶ï¼ˆDiscrete Action Spaceï¼‰
- **DQN** (Deep Q-Network): åŸºç¡€çš„æ·±åº¦ Q å­¦ä¹ ç®—æ³•
- **Double DQN**: ä½¿ç”¨åŒç½‘ç»œå‡å°‘ Q å€¼é«˜ä¼°

### è¿ç»­æ§åˆ¶ï¼ˆContinuous Action Spaceï¼‰
- **DDPG** (Deep Deterministic Policy Gradient): ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ç®—æ³•
- **TD3** (Twin Delayed DDPG): æ”¹è¿›çš„ DDPGï¼Œä½¿ç”¨åŒ Critic å’Œå»¶è¿Ÿç­–ç•¥æ›´æ–°

### æ¢ç´¢ç­–ç•¥
- **Epsilon-Greedy**: ç”¨äºç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆDQN ç³»åˆ—ï¼‰
- **Gaussian Noise**: ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆDDPG/TD3ï¼‰
- **Dummy Strategy**: ä¸æ·»åŠ æ¢ç´¢å™ªå£°ï¼ˆç”¨äº DDPG çš„ OU Noise å†…ç½®æ¢ç´¢ï¼‰

### å›æ”¾ç¼“å†²åŒº
- **ReplayBuffer**: å‡åŒ€é‡‡æ ·çš„ç»éªŒå›æ”¾
- **PrioritizedReplayBuffer**: åŸºäº TD è¯¯å·®çš„ä¼˜å…ˆçº§å›æ”¾

## ä¾èµ–è¦æ±‚

- Python â‰¥ 3.10
- PyTorch â‰¥ 2.9
- Gymnasium â‰¥ 1.2
- NumPy â‰¥ 2.2
- Matplotlib â‰¥ 3.10
- Draccus â‰¥ 0.11ï¼ˆé…ç½®ç®¡ç†ï¼‰
- PyBulletï¼ˆç”¨äºæœºå™¨äººç¯å¢ƒï¼Œå¯é€‰ï¼‰

å®Œæ•´ä¾èµ–åˆ—è¡¨è§ `pyproject.toml`ã€‚

## å‚è€ƒæ–‡çŒ®

- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) (DQN)
- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (DDQN)
- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)
- [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971) (DDPG)
- [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) (TD3)
